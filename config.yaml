# Training Configuration for ATG Model
# An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction

# Data Configuration
data_file: "dataset/scierc.pkl"

# Model Configuration
model_name: "scibert_cased"  # Available: spanbert, bert, roberta, scibert, arabert, bertlarge, scibert_cased, albert, spanbertlarge, t5-s, t5-m, t5-l, deberta
max_width: 14
num_prompts: 5
hidden_transformer: 512
num_transformer_layers: 6
attention_heads: 8
span_mode: "conv_share"  # Available: endpoints, attentive, marker, markconv, birectionnal, query, mlp, cat, conv_conv, conv_max, conv_mean, conv_sum, conv_share, conv_share_endpoints
p_drop: 0.1
use_pos_code: true
cross_attn: true

# Training Configuration
n_epochs: 1
n_steps: 10000
batch_size: 1
eval_batch_size: 1
warmup_ratio: 0.1
grad_accumulation_steps: 1
max_num_samples: 1

# Optimizer Configuration
lr_encoder: 1.0e-5
lr_decoder: 1.0e-4
lr_others: 5.0e-4

# Logging Configuration
save_interval: 1000
log_dir: null  # Will be required when running training

# Model Paths (customize as needed)
model_paths:
  spanbert: "/gpfswork/rech/pds/upa43yu/models/spanbert-base-cased"
  bert: "/gpfswork/rech/pds/upa43yu/models/bert-base-cased"
  roberta: "/gpfswork/rech/pds/upa43yu/models/roberta-base"
  scibert: "/gpfswork/rech/pds/upa43yu/models/scibert-base"
  arabert: "/gpfswork/rech/pds/upa43yu/models/bert-base-arabert"
  bertlarge: "/gpfsdswork/dataset/HuggingFace_Models/bert-large-cased"
  scibert_cased: "/gpfswork/rech/pds/upa43yu/models/scibert_cased"
  albert: "/gpfswork/rech/pds/upa43yu/models/albert-xxlarge-v2"
  spanbertlarge: "/gpfswork/rech/pds/upa43yu/models/spanbert-large-cased"
  t5-s: "/gpfsdswork/dataset/HuggingFace_Models/t5-small"
  t5-m: "/gpfsdswork/dataset/HuggingFace_Models/t5-base"
  t5-l: "/gpfsdswork/dataset/HuggingFace_Models/t5-large"
  deberta: "/gpfswork/rech/pds/upa43yu/models/deberta-v3-large"