# Training Configuration for ATG Model
# An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction

# Data Configuration
data_file: "dataset/conll04.pkl"

# Model Configuration
model_name: "PartAI/TookaBERT-Base"  # Persian language model - Available: spanbert, bert, roberta, scibert, arabert, bertlarge, scibert_cased, albert, spanbertlarge, t5-s, t5-m, t5-l, deberta, tookabert
max_width: 14
num_prompts: 5
hidden_transformer: 512
num_transformer_layers: 6
attention_heads: 8
span_mode: "conv_share"  # Available: endpoints, attentive, marker, markconv, birectionnal, query, mlp, cat, conv_conv, conv_max, conv_mean, conv_sum, conv_share, conv_share_endpoints
p_drop: 0.1
use_pos_code: true
cross_attn: true

# Training Configuration
n_epochs: 1
n_steps: 10000
batch_size: 1
eval_batch_size: 1
warmup_ratio: 0.1
grad_accumulation_steps: 1
max_num_samples: 1

# Optimizer Configuration
lr_encoder: 1.0e-5
lr_decoder: 1.0e-4
lr_others: 5.0e-4

# Logging Configuration
save_interval: 1000
log_dir: "./logs"  # Default log directory

# Model Paths (Hugging Face Hub model names)
model_paths:
  spanbert: "SpanBERT/spanbert-base-cased"
  bert: "google-bert/bert-base-cased"
  roberta: "FacebookAI/roberta-base"
  scibert: "allenai/scibert_scivocab_uncased"
  arabert: "aubmindlab/bert-base-arabert"
  bertlarge: "google-bert/bert-large-cased"
  scibert_cased: "allenai/scibert_scivocab_cased"
  albert: "albert/albert-xxlarge-v2"
  spanbertlarge: "SpanBERT/spanbert-large-cased"
  t5-s: "google-t5/t5-small"
  t5-m: "google-t5/t5-base"
  t5-l: "google-t5/t5-large"
  deberta: "microsoft/deberta-v3-large"
  tookabert: "PartAI/TookaBERT-Base"
